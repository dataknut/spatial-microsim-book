---
title: "Spatial microsimulation with R"
output:
  word_document: default
  pdf_document:
    fig_caption: yes
    includes:
      in_header: bookstyle.sty
    number_sections: yes
    toc: yes
layout: default
---

```{r, echo=FALSE}
# This is an early draft of a book on spatial microsimulation, for teaching in Cambridge
```


\clearpage

Introduction {#Introduction}
=======================

Spatial microsimulation is statistical technique for combining
individual-level datasets with geographical data and analysing the
resulting *spatial microdata*.
The term is little known outside the fields of human geography and
regional science. Yet the underlying methods have the potential to be
useful in a wide range of applications. Spatial microsimulation, as taught
in this book, can be of use to local housing administrators, transport planners
and researchers hammering out the details of how society could operate in a
post carbon world --- after we stop burning fossil fuels.

There is growing interest in spatial microsimulation. This is
due largely to its
practical utility in an era of 'evidence-based policy'
but is also driven by changes in the wider
research environment inside and outside of academia. Continuous
improvements in computers, software
and data availability mean spatial microsimulation is more accessible
than ever. It is now possible to
simulate the populations of small administrative areas
at the individual-level almost anywhere in the world. This opens new
possibilities for a range of applications, not least policy evaluation.

Still, the meaning of spatial microsimulation is ambiguous for many. This is
partly because the technique is inherently difficult to understand
and partly due to researchers themselves:
some uses of the term in the academic literature
are unclear or inconsistent about what the method entails.
Worse is work that treats spatial microsimulation
as a magical black box. This book is about demystifying spatial microsimulation.

Spatial microsimulation
can be understood either as a technique or an approach:

1. A *technique* for generating spatial microdata --- individuals allocated to
zones (see Figure 1).
2. An *approach* to modelling based on
spatial microdata, simulated or real.

```{r, fig.cap="Schematic of the spatial microsimulation technique", fig.width=4, fig.height=4, echo=FALSE}
library(png)
library(grid)
img <- readPNG("figures/msim-schema-lowres.png")
grid.raster(img)
```

Throughout this book we will see spatial microsimulation in
both senses of the term, generally moving from the former
to the latter perspective as the chapters progress.

Another issue tackled in this book is reproducibility.
Most findings in the field cannot easily be
replicated, meaning there is no way of independently checking the results.
In today’s age of fast Internet connections, open
access datasets and free software, there is little excuse for this.
The issue is not unique to the spatial microsimulation:
it is widespread in academia,
leading to calls such as that by Sergio Rey for an 'Open Regional Science'
([Rey, 2014](http://link.springer.com/10.1007/s00168-014-0611-7)).
Likewise, this book calls for 'Open Spatial Microsimulation'.

Reproducibility is encouraged throughout via
provision of the tools for readers to
actually *do* spatial microsimulation. Small yet realistic
datasets are provided to run the methods on own computer.
All the findings presented
in this book can therefore be
reproduced using code and data in the book's
Github repository:
[github.com/Robinlovelace/spatial-microsim-book](https://github.com/Robinlovelace/spatial-microsim-book).

Why spend time and effort on reproducibility? The first answer is that
reproducibility actually saves time in the long-run, by ensuring more
readable code and allowing your results to be easily re-run at a later data.
The second reason is more profound. Reproducibility is a prerequisite
of falsifiability and falsifiability is the backbone of science
(Popper, 1959).
The results on non-reproducible research cannot be verified, reducing scientific
credibility. These observations inform the book’s practical nature.

```{r, echo=FALSE}
# Why spend time and effort on reproducibility? The first answer is that
# reproducibility actually saves time in the long-run, by ensuring more
# readable code and allowing your results to be easily re-run at a later data.
# The second reason is more profound. Reproducibility is a prerequisite
# of falsifiability and falsifiability is the backbone of science
# (Popper, 1959).
# The results on non-reproducible research cannot be verified, reducing scientific
# credibility. These observations inform the book’s practical nature.
# The aim is simple: to provide a foundation in spatial microsimulation.
# http://en.wikipedia.org/wiki/Experiential_learning
# Poper's link also here
# [2011](http://www.manning.com/kabacoff/)
```

This book presents spatial microsimulation
as a living, evolving set of techniques rather
than a prescriptive formula for arriving at the 'right' answer.
<!-- The following doesn't make sense. In the meantime I've changed it but it
might not be what you want to say:
This approach that spatial microsimulation largely defined by its user-community,
made up of growing number of people worldwide. -->
Spatial microsimulation is largely defined by its user-community,
made up of growing number of people worldwide. In terms of reproducibility,
this book aims to contribute to the community by
encouraging collaboration, innovation
and rigour. The book aims to
make spatial microsimulation accessible to more people, with
a practical approach that encourages playing with the data and code.
As Kabakoff (2011 p. xxii) put it regarding R,
"the best way to learn is to experiment". 

## Why spatial microsimulation with R?

```{r, echo=FALSE}
# expressing oneself.^[This video introduces the idea of
# expressing oneself in [R](http://youtu.be/wki0BqlztCo)].
```

Software decisions have a major impact on flexibility,
efficiency and reproducibility of research.
Nearly three decades ago 
[Hölm (1987, p. 153)](http://www.jstor.org/stable/10.2307/490448)
observed that "little attention is paid to the choice of programming
language used" for microsimulation. This appears to be as true now as it was
then; software is rarely discussed in papers on the subject and there
are few mature spatial microsimulation packages.^[The
[Flexible Modelling Framework (FMF)](https://github.com/MassAtLeeds/FMF)
is a notable exception written in Java that can perform various modelling tasks.]
There are many factors additional
that should influence software selection including
cost, maturity, features and
speed of execution. Perhaps most important for busy researchers is
ease and speed of writing,
adapting and communicating the analysis. R excels in each of these areas.

```{r, echo=FALSE}
# Yet the software used has a lasting
# impact, including what can and cannot be done
# and opportunities for collaboration.  explains the choice of R.
# In my own research, for example, a conscious decision was made early on to use
# R. This had subsequent knock-on impacts on
# the features, analysis and even design of my simulations.
# There are hundreds computer programming languages and many of these
# are general purpose and 'Turing complete', meaning they could, with
# sufficient effort, perform spatial microsimulation (or any other
# numerical operation). So why choose R?
# ^[Speed
# of execution is an arguable exception, an issue that can be tackled
# by vectorisation (see [Appendix A](#apR)) and judicious use of add-on *R packages*.]
```

R is a *low-level* language compared with statistical programs
based on a strong graphical user interface (GUI) such as Microsoft
Excel and SPSS. 
R offers great flexibility in the design of
analysis and modelling, easily allowing
the creation of custom functions.
On the other hand, R is *high-level* compared with
general purpose languages such as C and Python.
Instead
of writing code to perform statistical operations
'from scratch', R users generally use pre-made functions. To calculate
the mean value of variable `x`, for example,
one would need to type 20 characters in Python: `float(sum(x))/len(x)`.^[The
`float` function is needed in case whole numbers are used. This
can be reduced to 13 characters with the excellent **NumPy** package:
`import numpy; x = [1,3,9]; numpy.mean(x)` would generate the
desired result. The R equivalent is `x = c(1,3,9); mean(x)`.]
In pure R 7 characters are sufficient: `mean(x)`.

```{r, echo=FALSE}
# One may argue that saving a few keystrokes while writing
# code is not a priority but it is certain
# that the time savings of being concise can be vast.
```

The example of calculating the mean in R and Python
illustrates a wider point:
R was *designed* to work with statistical data, so many functions
in the default R installation (e.g. `lm()`, to create a
linear regression model) perform statistical analysis 'out of the box'.
In agent-based modelling, the statistical analysis of results
often occupies more time than running the model
itself 
([Theile, 2014](http://www.jstatsoft.org/v58/i02/paper)).
The same applies to
spatial microsimulation, making R an ideal choice due to its statistical
anlysis capabilities.

Finally, R has an active and  growing user community. As a result there are
thousands of packages that extend R's capabilities by providing new functions
to the user and improvements are being added all the time.
The **ipfp** package, for example, was published in the summer of 2014 and
can greatly reduce the computational time taken for a key element of spatial
microsimulation process, as we shall see in *[Reweighting with ipfp](#ipfp)*.
Further information about why R is a good choice for spatial microsimulation
is provided in the [Appendix](#apR). The next section describes approaches
to learning R in general terms.

```{r, echo=FALSE}
# For speed-critical applications,
# R provides access to lower level languages. It
# is possible to say a lot in R in few lines of code,
# but it is also possible for users to create their own
# commands, allowing users complete control. 
# The reasons for using R for spatial
# microsimulation can be summarised by modifying
# the arguments put forward by Norman Matloff (2001)
# for using R in general. R is:
# 
# -  "the de facto standard among
#     professional statisticians", meaning that the spatial microsimulation
#     code can easily be modified to perform a variety of statistical operations.
#  
# 
# -   "a general
#     programming language, so that you can automate your analyses and
#     create new functions." This is particularly useful if you need to run the same
#     code in many different ways for many locations. In R, the computer
#     can be asked to iterate over as many combinations of model runs as desired.
# 
# -   open source, meaning its easy to share your code and reproduce your
#     findings anywhere in the world, without the worry of infringing copyright
#     licences. In work funded by the public, this also has a large benefit
#     in terms of education and the democratisation of research.
```

Learning the R language
-----------------------

Having learned a little about *why* R is a good tool for the job, it is
worth considering at this stage *how* R should be used.
It is useful to think of R not as a series of isolated
commands, but as an interconnected *language*.
As with learning Spanish or Chinese, frequent
practice, persistence and experimentation will ensure deep learning.

The most useful practical advice I can give is to organise your workflow.
Each project should have its own self-contained folder containing all
that is needed to replicate the analysis, except perhaps
large input datasets. This could include the
raw (unchanged) input data, a folder containing R code for analysis,
a folder for graphical outputs and a folder for data output.
To avoid clutter, it is sensible to arrange this content into
folders (thanks to Colin Gillespie for this tip): 

```
|-- book.Rmd
|-- data
|-- figures
|-- output
|-- R
|   |-- load.R
|   `-- parallel-ipfp.R
`-- spatial-microsim-book.Rproj
```

The example directory structure above is taken from an early version of this book.
It contains the document for the write-up (`book.Rmd` --- this could equally
be a `.docx` or `.tex` file) and RStudio's `.Rproj` file in the *source directory*.
The rest of the entities are folders: one for the input data, one for figures generated,
one for data outputs and one for R scripts. The R scripts should have meaningful
names and contain only code that works and is commented. An additional backup
directory could be used to store experimental code. There is no need to
be prescriptive in following this structure, but projects using spatial
microdata tend to be complex, so imposing order over your workflow early will
likely yield dividends in the long run.

The same applies to learning the R language.
Fluency allows
complex numerical ideas to be described with a small number of keystrokes.
If you are a new R user it is therefore worth spending some time learning
the R language. To this end
[the Appendix](#apR) provides a primer on R from the perspective
of spatial microsimulation.

```{r, echo=FALSE}
# Consider the following expression in the language of mathematics:
# 
# 
# 
# It is easy for experienced R users to translate this into R:
# 
# 
# 
# Note that although the R language is not quite as concise or elegant as
# mathematics, it is certainly faster at conveying the meaning of numerical
# operations than plain English and, in many cases, other programming languages.
# 
# 
# 
# The unusually concise nature of R code is not an accident. It was
# planned to be this way from the outset by its early developers, Robert
# Gentleman and Ross Ihaka, who thought carefully about syntax from the
# outset: "the syntax of a language is important because it determines the
# way that users of the language express themselves" (Ihaka and Gentleman, 2014, p. 300).
# 
# If you are new to R but have some experience with data analysis and
# microsimulation, do not feel intimidated that R is a foreign language.
# As with a spoken language, often the best way to learn is to
# 'jump in the deep end' by living abroad, so learning R through the course
# of this book is certainly an option. However, a deep understanding of R
# will greatly assist understanding the practical elements of the book which
# begin in earnest in [Chapter 4](#DataPrep). Therefore an introductory
# tutorial is provided in [Appendix 1](#apR) which will allow this book
# to focus primarily on the methods of spatial microsimulation and not the
# language in which they are implemented.
```

Typographic conventions
-----------------------

The following typographic conventions are followed to make the practical
examples easier to follow:

- In-line code is provided in `monospace` font to show it's something the
computer understands.
- Larger blocks of codes, referred to as *listings*, are provided on separate lines
and have coloured *syntax highlighting* to distinguish between values, names and functions:

```{r}
x <- c(1, 2, 5, 10) # create a vector
sqrt(x) # find the square root of x
```
 - Output from the *R console* is preceded by the `##` symbol, as illustrated above.
 - Comments are preceded by a single `#` symbol to explain specific lines.
 
There are many ways to write R code that will generate the same results.
However, to ensure clarity and consistency, a single style, advocated in
[Hadley Wickham](http://r-pkgs.had.co.nz/style.html)'s *Advanced R*
book ([Wickham, 2014](http://www.crcpress.com/product/isbn/9781466586963)),
is followed throughout.
Consistent style and plentiful
comments will make your code readable by yourself and others for decades to come.

An overview of the coursebook
-----------------------

This coursebook builds on the tutorial
*Introducing spatial microsimulation with R: a practical*
(Lovelace, 2014) with improved code and explanation.
The booklet is a precursor to a book in CRC Press's
[R Series](http://www.crcpress.com/browse/series/crctherser)
which will be published in summer 2015. Therefore any comments
on the code, explanation or contents will be gratefully received.^[Feedback
can be left via email to r.lovelace@leeds.ac.uk or via the project's GitHub page:
[github.com/Robinlovelace/spatial-microsim-book](github.com/Robinlovelace/spatial-microsim-book).]

The structure is as follows:

- [SimpleWorld](#SimpleWorld), a 'no nonsense' and reproducible
explanation of spatial microsimulation with reference to an imaginary planet.
- [Data preparation](#DataPrep), dedicated to the boring but vital task
of loading and 'cleaning' the input data, ready for spatial microsmulation.
- [Spatial microsimulation in R](#Smsim1), which introduces the main functions
and techniques that are used to generate spatial microdata.
- [CakeMap](#CakeMap), a larger and more involved example using real data.
- [Next steps](#NextSteps), an introduction to the subsequent steps that can
be taken after the spatial microdataset has been generated.

The majority of this material is new and will contribute to a textbook
on spatial microsimulation. Please contact r.lovelace@leeds.ac.uk with
any feedback about the book. Alternatively, if you have a GitHub account,
feel free to report any issues online and contribute directly to the project
by editing this documents source code: [github.com/Robinlovelace/spatial-microsim-book/blob/master/book-cambridge.Rmd](https://github.com/Robinlovelace/spatial-microsim-book/blob/master/book-cambridge.Rmd).

\clearpage

```{r, echo=FALSE}
# # What is spatial microsimulation?
# 
# Spatial microsimulation, as used in this book, is
# statistical technique for allocating individuals from a survey dataset
# to administrative zones based on shared variables between the areal and
# individual level data.
# 
# However, as with many new and infrequently used phrases, this
# understanding is not shared by everyone. The meaning of spatial
# microsimulation varies depending on context: to an
# economist spatial microsimulation is likely to imply
# modelling temporal processes such as how individual agents 
# respond to changes in prices or policies. To a transport
# planner, the term implies simulating the precise movements of vehicles on
# the transport network. To your next door neighbour it may mean you have
# started speaking gobbledygook! Hence the need to define our terminology.
# 
# Terminology
# -----------
# 
# Delving a little into the etymology and history of the term reveals the
# reasons behind this duplicity of meaning and highlights the importance
# of terminology. Only in very few contexts will one be understood when
# one says “I use *spatial microsimulation*” in everyday life. Usually it
# is important to add context. Below are a few hypothetical situations and
# suggestions of how one could respond to them.
# 
# -   When talking to a colleague, a transport modeller: “spatial
#     microsimulation, also known as population synthesis...”
# 
# -   Speaking to agent based modellers: “we use spatial microsimulation
#     to simulate the characteristics of geo-referenced agents...”
# 
# -   Communicating with undergraduates who are unlikely to have come
#     across the term or its analogies. “I do spatial microsimulation, a
#     way of generating individual-level data for small areas...”
# 
# -   Chatting casually in the pub or coffee shop: “I’m using a technique
#     called spatial microsimulation to model people...”.
# 
# The above examples illustrate that there is great potential for
# confusion and shows that care needs to be taken to tailor the words used
# depending on the target audience. All this links back to the importance
# of transparency and reproducibility of method discussed in .
# 
# Faced with uncomprehending stares when describing the method, some may
# be tempted to ‘blind them with science’, relying on
# sophisticated-sounding jargon, for example by saying: “we use simulated
# annealing in our integerised spatial microsimulation model”. Such
# wording obscures meaning (how many people in the room will understand
# ‘integerised’, let alone ‘simulated annealing’) and makes the process
# sound inaccessible. Although much jargon is used in the spatial
# microsimulation literature in this book, care must be taken to ensure
# that people understand what you are saying.
# 
# This raises the question, why use the term spatial microsimulation at
# all, if it is understood by so few people? The answer to this is that
# spatial microsimulation, defined clearly at the outset and used
# correctly, can usefully describe a technique that would otherwise need
# many more words on each use. Try replacing ‘spatial microsimulation’
# with ‘a statistical technique to allocate individuals from a survey
# dataset to administrative zones based on shared variables between the
# areal and individual level data’ each time it appears in this book and
# the advantages of a simple term should become clear. ‘Population
# synthesis’ is perhaps a more accurate term but, transport modelling
# aside, the literature already uses ‘spatial microsimulation’. Rather
# than create more complexity with *another* piece of jargon, it is best
# to continue with the term favoured by the the majority of practitioners.
# 
# Why has this situation, in which practitioners of a statistical method
# must tread carefully to avoid confusing their audience, come about?
# First it’s worth stating that the problem is by no means unique to this
# field: imagine the difficulties that Bayesian statisticians must
# encounter when speaking of prior and posterior probability distributions
# to an uninitiated audience. Let alone describing Gibb’s sampling. To
# more precisely answer the question, and gain an insight into the origins
# of the definition provided at the outset of this chapter, we consider
# the beginnings and evolution of the term in written work.
# 
# The etymology of spatial microsimulation
# ----------------------------------------
# 
# Spatial microsimulation as an approach to modelling {#sbroader}
# ---------------------------------------------------
# 
# What spatial microsimulation is not
# -----------------------------------
# 
# **Spatial microsimulation is not strictly spatial**
# 
# The most surprising feature of spatial microsimulation, as used in the
# literature, is that *the method is not trictly *spatial*. The only
# reason why the methodology has developed this name (as opposed to 'small
# area population synthesis', for example) is that practitioners tend
# to use administrative zones, which represent geographical areas, as the
# grouping variable. However, any mutually exclusive grouping variable,
# such as age band or number of bedrooms in your house, could
# be used. Likewise, geographical location can be used as a *constraint variable*.
# In most spatial microsimulation models, *the spatial variable is a mutually
# exclusive grouping, interchangeable with any such group*. "Spatial" is thus
# 1st on the list of things that spatial microsimulation is not.
# 
# To be more precise, spatial microsimulation is not *inherently spatial*.
# Spatial attributes such as the geographic coordinates of home and work
# locations can easily be added to the spatial microdata after they have been
# generated, and the use of geographical variables as the grouping variable is
# critical here. 
# 
# **Spatial microsimulation is not agent-based modelling (ABM).**
```

# SimpleWorld {#SimpleWorld}

To see the link between the methodology introduced
*[later in the book](#Smsim1)*
and the various real-world applications, let's take a look at a simple example of the
kind of situation where spatial microsimulation is useful,
to help bridge the gap between method
and application.

We'll use an imaginary world called SimpleWorld,
consisting of only 3 zones that cover the entirety of
the SimpleWorld sphere ([Figure 2](fsimple1)). 

```{r fsimple1, fig.cap="The SimpleWorld sphere", echo=FALSE, message=FALSE} 
# Code to create SimpleWorld
# Builds on this vignette: http://cran.r-project.org/web/packages/sp/vignettes/over.pdf
library(sp)
library(ggplot2)
xpol <- c(-180, -60, -60, -180, -180)
ypol <- c(-70, -70, 70, 70, -70)
pol = SpatialPolygons(list(
  Polygons(list(Polygon(cbind(xpol, ypol))), ID="x1"),
  Polygons(list(Polygon(cbind(xpol + 120, ypol))), ID="x2"),
  Polygons(list(Polygon(cbind(xpol + 240, ypol))), ID="x3")
  ))
# plot(pol)
proj4string(pol) <- CRS("+init=epsg:4326")
pol1 <- fortify(pol)

theme_space_map <- theme_bw() +
  theme(
#     rect = element_blank(),
    axis.title = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    panel.grid.major = element_line(size = 3)
  )

ggplot(pol1) + geom_path(aes(long, lat, group, fill = group)) +
    coord_map("ortho", orientation=c(41, -74, 52)) + 
  theme_space_map
```

This is a small world, 
containing 12, 10 and 11 individuals of its alien inhabitants
in each zone, 1 to 3, respectively. From the SimpleWorld Census, we know
how many young (under 49 space years old) and old (over 50)
residents live in each
zone, as well their genders: male and female.
This information is displayed in the tables below.

|zone   | 0-49 yrs| 50 + yrs|
|:--|-----:|-----:|
|1  |     8|     4|
|2  |     2|     8|
|3  |     7|     4|


|Zone   |  m|  f|
|:--|--:|--:|
|1  |  6|  6|
|2  |  4|  6|
|3  |  3|  8|

```{r fig = "Mercator maps of the zones in SimpleWorld", echo=FALSE, message=FALSE}
con_age <- read.csv("data/SimpleWorld/age.csv")
con_sex <- read.csv("data/SimpleWorld/sex.csv")
cons <- cbind(con_age, con_sex)

# library(knitr)
# kable(con_age, row.names = T)
# kable(con_sex, row.names = T)

pol <- SpatialPolygonsDataFrame(pol, cons, match.ID = F)

# pol@data
pol$p_young <- pol$a0.49 / (pol$a.50. + pol$a0.49) * 100
pol$p_male <- pol$m / (pol$f + pol$m) * 100

pol$id <- c("x1", "x2", "x3")
library(plyr)
pol1 <- join(pol1, pol@data)
pol1$Name <- paste("Zone", 1:3, sep = " ")
pol1$xpos <- seq(-120, 120, length.out = 3)
pol1$ypos <- 0

# ggplot(pol1) + 
#   geom_polygon(aes(long, lat, group, fill = p_young)) +
#   geom_path(aes(long, lat, group, fill = p_young)) +
#   geom_text(aes(xpos, ypos, label = Name)) +
#   theme_bw() +
#   scale_fill_continuous(low = "black", high = "white", limits = c(0, 100),
#     name = "% Young") +
#   coord_map() 
# 
# ggplot(pol1) + 
#   geom_polygon(aes(long, lat, group, fill = p_male)) +
#   geom_path(aes(long, lat, group, fill = p_male)) +
#   geom_text(aes(xpos, ypos, label = Name)) +
#   theme_bw() +
#   scale_fill_continuous(low = "black", high = "white", limits = c(0, 100),
#     name = "% Male") +
#   coord_map() 
```

Next, imagine a more detailed dataset about 5 of SimpleWorld's
inhabitants, recorded from a survey. This is in a different
form from the aggregate-level data presented in the above tables.
This *microdata* survey contains one row per individual, in contrast
to the *aggregate constraints*, which have one row per zone.
This individual level data includes exact age
(not just the crude and unflattering categories
of "young" and "old"), as well as income:

```{r, echo=FALSE}
# ind <- read.csv("data/SimpleWorld/ind.csv")
# ind$income <- round(rnorm(n = nrow(ind), mean = 1000, sd = 100))
# ind$income <- ind$income + 30 * ind$age
# ind$income[ ind$age == "f"] <- ind$income + 1000
# write.csv(ind, "data/SimpleWorld/ind-full.csv", row.names = F)
ind <- read.csv("data/SimpleWorld/ind-full.csv")
# kable(ind)
```

| id| age|sex | income|
|--:|---:|:---|------:|
|  1|  59|m   |   2868|
|  2|  54|m   |   2474|
|  3|  35|m   |   2231|
|  4|  73|f   |   3152|
|  5|  49|f   |   2473|

Note that although the microdataset contains additional information
about the inhabitants of SimpleWorld, it lacks geographical information
about where each inhabitant lives or even which zone they are from.
This is typical of individual-level survey data.
Spatial microsimulation tackles this issue by allocating individuals from
a non-geographical dataset to geographical zones in another.

The procedures we will learn to use in this book do this by allocating
*weights* to each individual for each zone. The higher the weight for a particular
individual-zone combination, the more representative that individual is of that
zone. This information can be represented as a *weight matrix*, such as the one
shown below.

```{r, echo=FALSE, eval=FALSE}
cat_age <- model.matrix(~ ind$age - 1)
cat_sex <- model.matrix(~ ind$sex - 1)[, c(2, 1)]
ind_cat <- cbind(cat_age, cat_sex) # combine flat representations of the data
ind$age <- cut(ind$age, breaks = c(0, 49, 120), labels = c("a0_49", "a50+"))
weights <- matrix(data = NA, nrow = nrow(ind), ncol = nrow(cons))
cons <- apply(cons, 2, as.numeric)
library(ipfp) # load the ipfp library after: install.packages("ipfp")
apply(cons, MARGIN = 1, FUN =  function(x) ipfp(x, t(ind_cat), x0 = rep(1,nrow(ind))))
```


| Individual| Zone 1  | Zone 2  | Zone 3  |
|---:|--------:|--------:|--------:|
|   1|    1.228|    1.725|    0.725|
|   2|    1.228|    1.725|    0.725|
|   3|    3.544|    0.550|    1.550|
|   4|    1.544|    4.550|    2.550|
|   5|    4.456|    1.450|    5.450|

The highest value (5.450) is located,
to use R's notation, in cell `weights[5,3]`,
the 5th row and 3rd column in the matrix `weights`. This means
that individual number 5
is considered to be highly representative of Zone 3,
given the input data in SimpleWorld.
This makes sense because there are many (7) young people
and many (8) females in Zone 3, relative to the input microdataset
(which contains only 1 young female). The lowest value (0.550)
is found in cell `[3,2]`. Again this makes sense: individual
3 from the microdataset is a young male yet there are
only 2 young people and 4 males in zone 2. A special
feature of the weight matrix above is that each of the column
sums is equal to the total population in each zone.
We will discover how the weight matrices are generated in
[a subsequent section](#Smsim1).

A more useful output from spatial microsimulation
is what we refer to as *spatial microdata*.
This is dataset that contains a single row per individual (as with the
input microdata) but also an additional variable indicating where each
individual lives. The challenge is to ensure that the spatial microdataset
is as representative as possible of the aggregate constraints, while only
sampling from a realistic baseline population. A feasible combination of
individuals sampled from the microdata that represent zone 2 is presented
in table xx below; the complete spatial microdataset allocates
whole individuals to each zone, resulting in a more or less realistic
insight into the inhabitants of SimpleWorld and for the purposes of
modelling. 

| id| age|sex | zone|
|--:|---:|:---|----:|
|  1|  59|m   |    2|
|  2|  54|m   |    2|
|  4|  73|f   |    2|
|  4|  73|f   |    2|
|  4|  73|f   |    2|
|  4|  73|f   |    2|
|  5|  49|f   |    2|
|  3|  35|m   |    2|
|  4|  73|f   |    2|
|  2|  54|m   |    2|

The table is a reasonable approximation of the inhabitants of zone 1:
older females dominate in both the aggregate (which contains 8 older people
and 6 females) and the simulated spatial microdata (which contains 8 older people
and 6 females). We will learn how to create such *integerised* datasets
during the course of this book.

```{r, echo=FALSE}
# Add section link here!
```

But how are these outputs *useful*?

Even though the datasets are tiny in SimpleWorld, we have already generated
some useful output. We can estimate, for example, the average income
in each zone. Furthermore, we could create an estimate of the *distribution*
of income in each area. Although these estimates are unlikely to be very
accurate due to the paucity of data, the methods could be very useful
if performed on larger datasets from the RealWorld (planet Earth).
Finally, the spatial microdata presented in the above table can be used
as an input into an agent-based model (ABM). Assuming the inhabitants of
SimpleWorld are more predictable than those of RealWorld, the outputs from
such a model could be very useful indeed, for example for predicting
future outcomes of current patterns of behaviour.

In addition to clarifying the advantages of spatial microsimulation,
the above example also flags some limitations
of the methodology: spatial microsimulation will only
yield useful results if the input microdataset is representative of the population
as a whole, and for each region. If the relationship between age sex is markedly different
in one zone compared with what we assume to be the global averages of the input data,
for example, our estimates could be way-out. Using such a small sample, one could
rightly argue, how could the diversity of 33 inhabitants of SimpleWorld be
represented by our simulated spatial microdata? This question is equally applicable
to larger simulations. These issues are important and will be tackled
in [section validation](#svalidation).

```{r, echo=FALSE}
# Applications of spatial microsimulation - to be completed!

## Updating cross-tabulated census data

## Economic forecasting

## Small area estimation

## Transport modelling

## Dynamic spatial microsimulation

## An input into agent based models
```

\clearpage

# Preparing input data {#DataPrep}

```{r, echo=FALSE}
# With the foundations built in the previous chapters now (hopefully) firmly in-place,
# we progress in this chapter to actually *run* a spatial microsimulation model
# This is designed to form the foundation of a spatial microsimulation course.
```


```{r, echo=FALSE}
# This next chapter is where people get their hands dirty for the first time -
# could be the beginning of part 2 if the book's divided into parts. 
```

This chapter focuses on the input datasets needed for spatial microsimulation.
Correctly loading, manipulating and assessing these datasets
will be critical to the performance of your models and the ease of modifying
them to include new inputs.
Fortunately R is an accomplished tool for data cleaning
([Kabacoff, 2011](http://www.manning.com/kabacoff/)), as we shall see.
This chapter also
provides the basis for chapter we perform spatial microsimulation.

As with most spatial microsimulation models, this example consists of a
non-geographical individual-level dataset and a series of geographical
zones. The data used in this chapter
(and the data for all other chapters) can be
downloaded from the book's GitHub repository
(). From this page, click on the 'Download ZIP' button to the right of the
page and extract the folder into a sensible place on your computer,
such as the Desktop.
From there, you will want to run R from the project's root directory:
open the folder in a file browser and double click on 'spatial-microsim-book.Rproj'.
This should cause RStudio to be opened at this location, with all the input
data files easily accessible to R through *relative filepaths*.

To ease reproducibility of the analysis when working with real data,
it is recommended that the
process begins with a copy of the *raw* dataset on one's hard disc.
Rather than modifying this file, modified ('cleaned') versions should be
saved as separate files. This ensures that after any mistakes,
one can always recover information that otherwise could have been lost and
makes the project fully reproducible. In this chapter, however,
a relatively clean and very tiny dataset from SimpleWorld is used.
We will see in [Chapter 5](#CakeMap) how to deal with larger and more messy data.
Here the focus is on the principles.

```{r, echo=FALSE}
# It sounds trivial, but the *precise* origin of the input data
# should be described. Comments in code that loads the data (and resulting publications),
# allows you or others to recall the raw information. # going on a little -> rm
# Show directory structure plot from Gillespie here
```

The process
of loading, checking and preparing the input datasets for spatial microsimulation
is generally a linear process, encapsulating the following stages:

1. Load original data 
2. Remove excess information
3. Re-categorise individual-level data
4. Set variable and value names
5. 'Flatten' individual-level data 

'Stripping down' the datasets so that they
only contain the bare essential information will enable you to focus solely
on the data that you are interested in. This is not covered in this chapter
because the input datasets are already extremely bare and because the
process should be obvious.

We start with the individual-level
dataset for a reason: this dataset is often more problematic
to format than the constraint variables, so it is worth becoming
acquainted with it at the outset. Of course, it is possible that
the data you have are not suitable for spatial microsimulation because
they lack sufficient constraint variables with shared categories in both
individual and aggregate level tables. We assume that you have already checked
this. The checking process for the datasets used in this chapter is simple:
both aggregate and individual-level tables contain age and sex, so they can
by combined. Let us proceed to load some data saved on our hard disc into
R's *environment*, where it is available in RAM.

## Loading input data {#Loading} 

Real-world individual-level data may be provided in a variety of formats
but ultimately needs to be loaded into R as a *data frame* object.

In this case the dataset is loaded from a `.csv` file:

```{r}
# Load the individual-level data
ind <- read.csv("data/SimpleWorld/ind.csv") 
class(ind) # verify the data type of the object
ind # print the individual-level data
```

```{r, echo=FALSE}
### Loading and checking aggregate-level data
```

Constraint data are usually made available one variable at a time,
so these are read in one file at a time:

```{r}
con_age <- read.csv("data/SimpleWorld/age.csv")
con_sex <- read.csv("data/SimpleWorld/sex.csv")
```

We have loaded the aggregate constraints. As with the individual level data,
is worth inspecting each object to
ensure that they make sense before continuing. Taking a look at `age_con`,
we can see that this data set consists of 2 variables for 3 zones:

```{r}
con_age
```

This tells us that there 12, 10 and 11 individuals in zones 1, 2 and 3,
respectively, with different proportions of young and old people. Zone
2, for example, is heavily dominated by older people: there are 8 people over
50 whilst there are only 2 young people (under 49) in the zone.

Even at this stage there is a potential for errors to be introduced.
A classic mistake with areal data is that the order in which zones are loaded
changes from one table to the next. The constraint data should come with some
kind of *zone id*, an identifying code that will eventually allow the attribute
data to be combined with polygon shapes in GIS software.

```{r, echo=FALSE}
# Make the constraint data contain an 'id' column, possibly scrambled 
```

If we're sure that the row numbers
match between the age and sex tables (we are sure in this case),
the next important test is to check
that the total populations are equal for both sets of variables.
Ideally both the *total* study area populations and *row totals*
should match. If the *row totals* match, this is a very good sign
that not only confirms that the zones are listed in the same order,
but also that each variable is sampling from the same *population base*
These tests are conducted in the following lines of code:

```{r}
sum(con_age)
sum(con_sex) 

rowSums(con_age)
rowSums(con_sex)
rowSums(con_age) == rowSums(con_sex)
```

The results of the previous operations are encouraging. The total population
is the same for each constraint overall and for each area (row) for both constraints.
If the total populations between constraint variables do not match (e.g. because
the sample population is different) this is problematic.
Appropriate steps to normalise
the errant constraint variables are described in
[the CakeMap chapter](#CakeMap).

## Subsetting to remove excess information

In the above code, `data.frame` objects containing precisely
the information required for the next stage were loaded.
More often, superfluous information will need to be removed from the data
and subsets taken. It is worth removing superfluous
variables earl, to avoid over-complicating and slowing-down the analysis.
If `ind` had 100 variables of which only the 1st, 3rd and 4th were of interest, for example,
the following command could be used to update the object, retaining only the relevant
variables: `ind <- ind[, c(1, 3, 4)]`. Alternatively, `ind$age <- NULL` removes
the age variable.

Although `ind` is small and simple it will behave in the same way as a much
larger dataset, providing opportunities for testing subsetting syntax in R.
It is common, for example,
to take a subset of the working *population base*: those
aged 16 and 74 in
full-time employment. Methods for doing this are provided in the
[the Appendix on subsetting](#subsetting).

## Re-cateorising individual-level variables

Before transforming the individual-level dataset `ind` into a form that
can be compared with the aggregate-level constraints, we must ensure that
each dataset contains the same information. It is more challenging to re-categorise
individual-level variables than to re-name or combine aggregate-level variables,
so the former should usually be set first.
An obvious difference between the individual and aggregate versions of the
`age` variable is that the former is of type `integer` whereas the latter is
composed of discrete bins: 0 to 49 and 50+. We can categories the variable
into these bins using `cut()`:^[The combination of curved and square brackets in the output may seem strange
but this is in fact an International Standard - see
[wikipedia.org/wiki/ISO_31-11](http://en.wikipedia.org/wiki/ISO_31-11) for more
information.]

```{r}
# Test binning the age variable
cut(ind$age, breaks = c(0, 49, 120))
```

If we wanted to change these category labels to something more readable,
we can do this by adding another argument to the `cut` function:

```{r}
# Convert age into a categorical variable with user-chosen labels
(ind$age <- cut(ind$age, breaks = c(0, 49, 120), labels = c("a0_49", "a50+")))
```

Users should be ware that `cut` results in a vector of class *factor*, which
can cause problems later down the line.

```{r}
names(cons)
```

## Matching individual and aggregate level data names

Before combining the newly recategorised individual-level data with the
aggregate constraints, it is useful to for the category labels to match up.
This may seem trivial, but will save time in the long run. Here is the problem:

```{r}
levels(ind$age)
names(con_age)
```

Note that the names are subtly different. To solve this issue, we can
simply change the names of the constraint variable, assuming they
are in the correct order:

```{r}
names(con_age) <- levels(ind$age) # rename aggregate variables
```

With both the age and sex constraint variable names now matching the
category labels of the individual-level data, we can proceed to create a
single constraint object we label `cons`. We do this with `cbind()`:

```{r}
cons <- cbind(con_age, con_sex)
cons[1:2, ] # display the constraints for the first two zones
```

## 'Flattening' the individual level data

We have made steps towards combining the individual and aggregate datasets and
now only need to deal with 2 objects (`ind` and `cons`) which now share
category and variable names.
However, these datasets cannot possibly be compared because they are of different
dimensions:

```{r}
dim(ind)
dim(cons)
```

The above code confirms this: we have one individual-level dataset comprising 5
individuals with 3 variables (2 of which are constraint variables) and one
aggregate-level constraint table called `cons`.
To re-cap, the constraints represent 3 zones
with count data for 4 categories across 2 variables.

The dimensions of at least one of these objects must change
before they can be easily compared. To do this
we 'flatten' the individual-level dataset;
this means increasing its width
to match the the constraint data.
This is a two-stage process. First,
`model.matrix()` is used to expand each variable into the number of columns as there are categories in each and assign. 
[Knoblauch and Maloney (2012)](http://www.springer.com/statistics/computational+statistics/book/978-1-4614-4474-9) provide a lengthier description of this
which is available [online for free](http://www.springer.com/statistics/computational+statistics/book/978-1-4614-4474-9).

Second, `colSums()` is used to take the sum of each column.^[As we shall see in [a subsequent section](#ipfp), only the former of these is needed if we use the
**ipfp** package for re-weighting the data, but both are presented to enable
a better understanding of how IPF works.]

```{r}
cat_age <- model.matrix(~ ind$age - 1)
cat_sex <- model.matrix(~ ind$sex - 1)[, c(2, 1)]
(ind_cat <- cbind(cat_age, cat_sex)) # combine flat representations of the data
```

Note that second call to `model.matrix` is suffixed with `[, c(2, 1)]`.
This is to swap the order of the columns: the column variables are produced
from `model.matrix` is alphabetic, whereas the order in which the variables
have been saved in the constraints object `cons` is `male` then `female`.
Such subtleties can be hard to notice yet completely change one's results
so be warned: the output from `model.matrix` will not always be compatible
with the constraint variables.

To check that the code worked properly,
let's count the number of individuals
represented in the new `ind_cat` variable, using `colSums`:

```{r}
colSums(ind_cat) # view the aggregated version of ind
ind_agg <- colSums(ind_cat) # save the result
```

The sum of both age and sex variables is 5 
(the total number of individuals): it worked! 
Looking at `ind_agg`, it is also clear that it has the same dimension as
each row in `cons`, the aggregate-level data. We can check this by inspecting
each object (e.g. via `View (ind_agg)`), although a more rigorous test is to see
if `ind_agg` can be combined with `ind_agg`, using `rbind`:

```{r}
rbind(cons[1,], ind_agg)
```

If no error message is displayed, the answer is yes.
This shows us a direct comparison between the number of people in each
category of the constraint variables in zone and and in the individual level
dataset overall. Clearly, the fit is not very good, with only 5 individuals
in total existing in `ind_agg` (the total for each constraint) and 12
in zone 1. We can measure the size of this difference using measures of
*goodnes of fit*. A simple measure is total absolute error (TAE), calculated in this
case as `sum(abs(cons[1,] - ind_agg))`: the sum of the positive differences
between cell values in the individual and aggregate level data.

The purpose of the *reweighting* procedure in spatial microsimulation is
to minimise this difference (as measured in TAE above)
by adding high weights to the most representative individuals.

\clearpage

# Spatial microsimulation in R {#Smsim1}

In this chapter we progress from loading and preparing the input data to
running a spatial microsimulation model.
The SimpleWorld data, loaded in the previous chapter,
is used. Being small and simple, the example enables understanding
the process on a 'human scale' and allows experimentation
without the worry of overloading your computer.
However, the methods apply equally to larger and more complex projects.
Therefore practicing the basic principles and methods of spatial microsimulation
in R is the focus of this chapter.
Time spent mastering these basics will make subsequent steps
much easier.

```{r, echo=FALSE}
# How representative each individual is of each zone is determined by their
# *weight* for that zone. If we have `nrow(cons)` zones and `nrow(ind)`
# individuals (3 and 5, respectively, in SimpleWorld) we will create
# 15 weights. Real world datasets (e.g. that presented in chapter xxx)
# could contain 10,000 individuals
# to be allocated to 500 zones, resulting in an unwieldy 5 million element
# weight matrix but we'll stick with the SimpleWorld dataset here for simplicity.
```

How representative each individual is of each zone is determined by their
*weight* for that zone. If we have `nrow(cons)` zones and `nrow(ind)`
individuals (3 and 5, respectively, in SimpleWorld) we will create
15 weights. Let us create an empty weight matrix, ready to be filled
with numbers calculated through the IPF procedure:

```{r}
weights <- matrix(data = NA, nrow = nrow(ind), ncol = nrow(cons))
dim(weights) # the dimension of the weight matrix: 5 rows by 3 columns
```

## IPF in R {#IpfinR}

<<<<<<< HEAD
One of the simplest ways to allocate the individual-level data loaded in
the previous chapter to the three zones of SimpleWorld is using iterative
proportional fitting (IPF). IPF is an established technique with a long history.
Interested readers are directed towards Lovelace and Ballas
([2012](http://www.sciencedirect.com/science/article/pii/S0198971513000240))
and Pritchard and Miller
([2012](http://link.springer.com/article/10.1007%2Fs11116-011-9367-4))
for recent work in this area, which
contain links to the theory and method underlying the method.
=======
The most established *deterministic* method
to allocate individuals to zones
is iterative proportional fitting (IPF).
IPF involves calculating a series of weights that represent
how representative each individual is of each zone.
This is *reweighting*. The IPF algorithm can be written in
R from scratch, as illustrated in Lovelace (2014).
However, to save computer and researcher time, we will
skip directly to an alternative method that uses the
**ipfp** package.

In any case, IPF can be used to allocate the individual-level data loaded in
the previous chapter to the three zones of SimpleWorld. IPF is mature,
fast and has a long history.
Interested readers are directed towards recent papers (e.g. Lovelace and Ballas
[2012](http://www.sciencedirect.com/science/article/pii/S0198971513000240);
Pritchard and Miller
[2012](http://link.springer.com/article/10.1007%2Fs11116-011-9367-4))
for more detail on the method and its underlying theory.
>>>>>>> upstream/master

```{r, echo=FALSE}
# Possibly more on IPF here. For now, press on
```



## Reweighting with **ipfp** {#ipfp}

<<<<<<< HEAD
It is possible to perform IPF
much faster and with less code than illustrated above using the
**ipfp** R package. The `ipfp` command that
implements the IPF algorithm
in the C language, as illustrated below on the same dataset:
=======
IPF runs much faster and with less code using the
**ipfp** package than in pure R. The `ipfp` function runs the IPF algorithm
in the C language, taking aggregate constraints, individual level
data and and an initial weight vector (`x0`) as inputs:
>>>>>>> upstream/master

```{r}
library(ipfp) # load the ipfp library after: install.packages("ipfp")
cons <- apply(cons, 2, as.numeric) # convert matrix to numeric data type
ipfp(cons[1,], t(ind_cat), x0 = rep(1,nrow(ind))) # run IPF
```

It is impressive that the entire IPF process, which takes dozens of lines of
code in pure R can been condensed into two lines: one to
convert the input constraint dataset to `numeric`^[The integer data type fails
because C requires `numeric` data to be converted into its *floating point*
data class.]
and one to perform the IPF operation itself. Note also that although
we did not specify how many iterations to run, the above command
ran the default of `maxit = 1000` iterations, despite convergence happening after
10 iterations. This can be seen by specifying the `maxit` and `verbose` arguments
in `ipfp`, as illustrated below (only the first line of R output is shown):

```{r, eval=FALSE}
ipfp(cons[1,], t(ind_cat), rep(1, nrow(ind)), maxit = 20, verbose = T)
```

```
## iteration 0:   0.141421
## iteration 1:	  0.00367328
```

Notice also that a *transposed* (via the `t()` function) version of the individual-level
data (`ind_cat`) is used in `ipfp`
to represent the individual-level data, instead of the
`ind_agg` object used in the pure R version. To prevent having to transpose
`ind_cat` every time `ipfp` is called, save the transposed version:

```{r}
ind_catt <- t(ind_cat) # save transposed version of ind_cat
```

Another object that can be saved prior to running `ipfp` on all zones
(the rows of `cons`) is `rep(1, nrow(ind))`, simply a series of ones.
We will call this object `x0` as it's argument name representing
the starting point of the weight estimates in `ipfp`:

```{r}
x0 <- rep(1, nrow(ind)) # save the initial vector
```

To extend this process to all three zones we can wrap the line beginning
`ipfp(...)` inside a `for` loop, saving the results each time into the
weight variable we created earlier:

```{r}
for(i in 1:ncol(weights)){
  weights[,i] <- ipfp(cons[i,], ind_catt, x0, maxit = 20)
}
```

To make this process even more concise (albeit
less clear to R beginners), we can use R's internal
`for` loop: `apply`:

```{r}
weights <- apply(cons, 1, function(x) ipfp(x, ind_catt, x0, 20))
```

In the above code R iterates through each row
(hence the second argument `MARGIN` being `1`: `MARGIN = 2`
would signify column-wise iteration).
Thus `ipfp` is applied to each zone in turn. 
The speed savings of writing the function
with different configurations are benchmarked in
'parallel-ipfp.R' in the 'R' folder of the book project directory.
This shows that reducing the maximum iterations of `ipfp` from
the default 1000 to 20 has the greatest performance benefit.^[These
tests also show that any speed gains from using `apply` instead of `for` are negligible, so
whether to use `for` or `apply` can be decided by personal preference.]
To make the code run faster on large datasets, a parallel version of
`apply` called `parApply` can be used. This is also
tested in 'parallel-ipfp.R'.

```{r, echo=FALSE, eval=FALSE}
# Also discuss what happens when you get a huge dataset, from Stephen's dataset
```

## Combinatorial optimisation

Combinatorial optimisation is an alternite to IPF for allocating individuals
to zones. This strategy is *probabilistic*
and results in integer weights, as opposed to the fractional weights
generated by IPF. Combinatorial optimisation is likely to be more appropriate
for applications where input individual microdatasets are very large:
speed benefits of using the deterministic IPF algorithm shrink as the size of
the survey dataset increases.

We will not cover combinatorial optimisation. Harland
([2013](http://eprints.ncrm.ac.uk/3177/)) provides a practical tutorial
introducing the subject based on the Flexible Modelling Framework (FMF)
Java program. It is possible to generate spatial microdata using IPF, however,
using a technique we refer to imaginatively as integerisation.
This is the topic of the next section.

## Integerisation

Integerisation is the process by which a vector of real numbers
is converted into a vector of integers corresponding to the
individuals present in synthetic spatial microdata.
The length of the new vector must equal the population of the zone
in question and individuals with high weights must be sampled
proportionally more frequently than those with
low weights for the operation to be effective.
The following example illustrates how the process,
when seen as a function called $int$ would work
on a vector of 3 weights:

$$w_1 = (0.333, 0.667, 3)$$

$$int(w_1) = (2, 3, 3, 3)$$

This result was obtained by calculating the sum of the weights (4, which
represents the total population of the zone) and sampling from these
until the total population is reached. In this case individual 2 is selected
once as they have a weight approaching 1, individual 3 was replicated
(*cloned*) three times and individual 1 does not appear in the integerised
dataset at all, as it has a low weight. In this case the outcome is straightforward
because the numbers are small and simple. But what about in less clear-cut cases,
such as $w_2 = (1.333, 1.333, 1.333)$? What is needed is an algorithm to undertake this
process of *integerisation* in a systematic way to maximise the fit between
the synthetic and constraint data for each zone.

In fact there are a number of integerisation strategies available.
Lovelace and Ballas (2012) tested 5 of these and found that probabilistic
integerisation methods consistently outperformed deterministic rivals.
The details of these algorithms are described in the aforementioned paper
and code is provided in the Supplementary Information. For the purposes of
this course we will create a function to undertake the simplest of these,
*proportional probabilities*:

```{r}
int_pp <- function(x){
  sample(length(x), size = round(sum(x)), prob = x, replace = T)
}
```

To test this function let's try it on the vectors of length 3 described in
code:

```{r}
set.seed(0)
int_pp(x = c(0.333, 0.667, 3))
int_pp(x = c(1.333, 1.333, 1.333))
```

The first result was the same as that obtained through intuition; the second
result represented individual 1 being clones three times, plus one instance of individual 2. This is
not intuitive: one would expect at least one of each individual given that they all
have the same weight.

It is important to emphasise that the results will change each time
the code is run, because `sample` is a probabilistic (its output depends on a random number generator):
changing the the value inside the brackets proceeding `set.seed`
results in many other combinations of individuals being selected --- test this out in your code.
This happens because the method relies on *pseudo random numbers* to select values
probabilistically and `set.seed` specifies where the random number sequence
should begin, ensuring repeatability. An issue with the
*proportional probabilities* (PP) strategy is that completely unrepresentative
combinations of individuals have a non-zero probability of being sampled:
the method will output $(1, 1, 1, 1)$ once in every 21 thousand
runs for $w_1$ and once every $81$ runs for $w_2$, the same probability
as for all other 81 ($3^4$) permutations.

To overcome this issue Lovelace and Ballas (2012) developed a method
which ensures that any individual with a weight above 1 would be sampled
at least, making the result $(1, 1, 1, 1)$ impossible in both cases.
This method is *truncate, replicate, sample* (TRS) integerisations:

```{r}
int_trs <- function(x){
  truncated <- which(x >= 1)
  replicated <- rep(truncated, floor(x[truncated]))
  r <- x - floor(x)
  def <- round(sum(x)) - length(replicated) # the deficit population
  if(def == 0){
    out <- replicated
  } else {
    out <- c(replicated, sample(length(x), size = def, prob = r, replace = F))
  }
  out
}
```

To see how this new integerisation method and associated R function
performed, we will run it on the same input vectors:

```{r}
int_trs(c(0.333, 0.667, 3))
int_trs(c(1.333, 1.333, 1.333))
```

The range of possible outcomes is smaller using the TRS technique;
the fit between the resulting microdata and the aggregate constraints
will tend to be higher. Thus we use the TRS methodology,
implemented through the function `int_trs`, for integerising the
weights generated by IPF throughout the majority of this book.

To gain an insight into the importance of integerisation,
let's use TRS to generate spatial microdata for SimpleWorld.
Remember, we already have generated the weight matrix `weights`.
The only challenge is to save the vector of sampled individual id numbers,
alongside the zone number, into a single object from which the attributes
of these individuals can be recovered. Two strategies for doing this are presented
in the code below:

```{r}
# Method 1: using a for loop
ints_df <- NULL
for(i in 1:nrow(cons)){
  ints <- int_trs(weights[, i])
  ints_df <- rbind(ints_df, data.frame(id = ints, zone = i))
}

# Method 2: using apply
ints <- unlist(apply(weights, 2, int_trs)) # generate integerised result
ints_df <- data.frame(id = ints, zone = rep(1:nrow(cons), colSums(weights)))
```

Both methods yield the same result for `ints_df`. The only differences
being that Method 1 is perhaps more explicit and easier to understand
whilst Method 2 is more concise.

The final remaining step is to re-allocate the attribute data from the
individual-level data back into `ints_df`. To do this we use the
`inner_join` function from the recently released **dplyr** package:^[The functions
`merge` from the R's base package and `join` from the **plyr** provide
other ways of undertaking this step. `inner_join` is used in place of
`merge` because `merge` does not maintain row order.
`join` generates the same result, but is slower, hence the use
of `inner_join` from the recently released and powerful **dplyr** package.]

```{r, message=FALSE}
ind_full <- read.csv("data/SimpleWorld/ind-full.csv")
library(dplyr) # install.packages(dplyr) if not installed
ints_df <- inner_join(ints_df, ind_full)
```

`ints_df` represents the final spatial microdataset, representing the
entirety of SimpleWorld's population of 33 (this can be confirmed with
`nrow(ints_df)`). To select individuals from one zone only is simple using
R's subsetting notation. To select all individuals generated for zone 2, 
for example, the following code is used. Note that this is the same as the
output generated in Table 5 at the end of the SimpleWorld chapter --- we have
successfully modelled the inhabitants of a fictional planet, including income!

```{r}
ints_df[ints_df$zone == 2, ]
```

\clearpage

# CakeMap: spatial microsimulation in the wild {#CakeMap}

By now we have developed a good understanding of what spatial microsimulation
is, its applications and how it works. We have seen
a little of both its underlying theory
and its implementation in R. However, we have yet to see how the method can
be applied *in the wild*, on *real* datasets.

"This spatial microsimulation technique seems useful, but how can I use
these methods on *my* data?" The purpose of this chapter is to answer this question
using a real dataset. 
The chapter is based on a hypothetical use of spatial microsimulation
to arrive at an important result:
estimated cake consumption in different parts of Leeds, UK.
The example is deliberately rather
absurd to make it more memorable and is
presented in a generalisable way, easing the transfer
of the method to new datasets.

## Preparing the input data

Often spatial microsimulation is presented in a way that suggests
the data arrived in a near perfect state, ready to be inserted directly into
the model. This is rarely the case in practice. Usually, one must
spend time translating the data into a suitable format, re-coding categorical
variables and column names, binning continuous variables and subsetting from
the microdataset. In a typical project, data preparation
can and should take as long as the analysis stage
([Wickham, 2014](http://vita.had.co.nz/papers/tidy-data.html)).
This section builds on
[Chapter 3](#DataPrep) to illustrate strategies for data cleaning
on a complex project. To learn about the data
cleaning steps that may be useful to your data, we start from the beginning
in this section, with a real (anonymised) dataset that was downloaded from
the internet.

```{r, echo=FALSE}
# A little long-winded - cut down?
```

The most difficult input dataset to deal with is the age/sex
constraint data. The steps used to clean it are saved in
'process-age.R', in the `data/CakeMap/` folder. Note that files to
load and reformat the other datasets are also present in this
folder, such 'process-age.R', 'process-nssec.R' and 'process-car.R'.
Take a look through [this file](https://github.com/Robinlovelace/spatial-microsim-book/blob/master/data/CakeMap/process-age.R)
and try to work out what is going on.

The end result of 'process-age.R' is a 'clean' .csv
file, ready to be loaded and used as the input into our spatial
microsimulation model. Note that the last line of
'process-age.R' is `write.csv(con1, "con1.csv", row.names = F)`.
This is the first constraint that we load into R to reweight the
individual-level data in the next section.

The input data generated through this process of data preparation are named
'con1.csv' to 'con3.csv'. For simplicity, all these were merged
(by '[load-all.R](https://github.com/Robinlovelace/spatial-microsim-book/blob/master/data/CakeMap/load-all.R)')
into a single
dataset called 'cons.csv'. All the input data for this section
are thus loaded with only two lines of code:

```{r}
ind <- read.csv("data/CakeMap/ind.csv")
cons <- read.csv("data/CakeMap/cons.csv")
```

Take a look at these input data using the techniques learned in the previous
section. To test your understanding, try to answer the following questions:

- What are the constraint variables?
- How many individuals are in the survey microdataset?
- How many zones will we generate spatial microdata for?

For bonus points that will test your R skills as well as your practical knowledge
of spatial microsimulation, try constructing queries in R that will automatically
answer these questions.

It is vital to understand the input datasets before trying to model them, so
take some time exploring the input. Only when satisfied with your understanding of
the datasets (a pen and paper can help here, as well as R!) is it time
to move on to generate the spatial microdata using IPF.

## Performing IPF on CakeMap data

The `ipfp` reweighting strategy is concise, generalisable and
computationally efficient. On a modern laptop, the `ipfp` method
was found to be *almost 40 times faster* than the 'IPFinR' method
(mentioned in section 4.1) over 20 iterations,
completing in 2 seconds instead of over 1 minute.
This is a huge time saving!^[These
tests can be run using the `microbenchmark()` commands found towards
the end of the 'CakeMap.R' file, is located
in the [source directory](https://github.com/Robinlovelace/spatial-microsim-book/blob/master/CakeMap.R) of the project. The second of these benchmarks depends
on files from `smsim-course`, the repository of which can
be downloaded from
[github.com/Robinlovelace/smsim-course](https://github.com/Robinlovelace/smsim-course).]

After some preparatory steps, described in chapter 4 and implemented in the
first half of 'CakeMap.R', the IPF stage is run using the following command:

```{r, eval=FALSE}
weights <- apply(cons, 1, function(x) ipfp(x, ind_catt, x0, 20))
```

```{r, echo=FALSE}
source("CakeMap.R")
```

## Integerisation

As before, weights of the IPF procedure are fractional, so must be
*integerised* to create whole individuals. The code presented in
chapter 4 requires little modification to do this: it is your task to
convert the weight matrix generated by the above lines of code into
a spatial microdataset called, as before, `ints_df`.

What is the size of this new object?
Clearly, this data takes up much RAM on the computer, as can be seen by
asking `object.size(intall.df)`. Try comparing this result with the
size of the original survey dataset 'ind'. Keeping an eye on
these parameters will ensure that the model does not generate
datasets too large to handle - a process that can easily happen.
Next we move on to a vital consideration in
spatial microsimulation models such as CakeMap: validation.

## Model checking and validation {#svalidation}

To make an analogy with food safety standards, openness about mistakes is
conducive to high standards. Transparency in model
verification is desirable for similar reasons. The two main strategies are:

1. comparing the model results with knowledge of how it *should*
perform {a-priori} (model checking), and
1. comparison between the model
results and empirical data (validation).

The total absolute error
(TAE), is a commonly used measure of the fit between two aggregate-level
datasets and which is defined by the following formula:

$$ TAE = \sum\limits_{ij}|U_{ij} - T_{ij}| $$

Standardised Total Error is a related measure: $SAE = TAE/P$
where $P$ is the total population of the study area.
Thus TAE is sensitive to the number of people
within the model. SAE is not.

Beyond typos or simple conceptual errors in model code, more fundamental
questions should be asked of spatial microsimulation models. The validity
of the assumptions on which they are built and the confidence one should have
in the results are important. For this we need external datasets.
Validation is therefore a tricky topic, something not covered
here but which is discussed in Edwards et al. (2009). For
more on this and  for (an albeit unreliable) comparison
between estimated cake consumption and external income estimates.

## Visualisations

Visualisation is an important part of communicating quantitative
data, especially so when the datasets are large and complex so not
conducive to description with tables or words.

Because we have generated spatial data, it is useful to create 
a map of the results, to see how it varies from place to place.
The code used to do this found in 'CakeMapPlot.R'. A vital function within
this script is the `inner_join` function.
Assuming **dplyr** is loaded --- with `library(plyr)` --- one can read
more about join by entering `?inner_join` in R.


```{r, eval=FALSE}
wardsF <- inner_join(wardsF, wards@data, by = "id")
```

The above line of code by default selects all the data
contained in the first object (`wardsF`) and adds to it new variables
from the second object based on the linking variable.
Also in that script file you will encounter the function
`fortify`, the purpose of which is to convert the spatial data object into a
data frame.
The final map result of `CakeMapPlot.R' is illustrated below.

![](figures/CakeMap.png)

**CakeMap results, visualised**

## Analysis and interpretation

Once a spatial microdataset has been generated that we are happy with,
we will probably want to analyse it further.
This means exploring it --- its main features,
variability and links with other datasets. To illustrate this process we will
load an additional dataset and compare it with the
estimates of cake consumption per person  generated in the previous
section at the ward level.  

The hypothesis we would like to test is that
cake consumption is linked to deprivation:
More deprived people will eat unheathily and
cake is a relatively cheap 'comfort food'.
Assuming our simulated data is correct
---  a questionable assumption but lets roll with
it for now --- we can explore this at the ward level thanks to a 
[dataset](http://www.neighbourhood.statistics.gov.uk/dissemination/instanceSelection.do?JSAllowed=true&Function=&%24ph=61&CurrentPageId=61&step=2&datasetFamilyId=266&instanceSelection=121427&Next.x=22&Next.y=13&nsjs=true&nsck=false&nssvg=false&nswid=1920) on 
modelled income from neighbourhood statistics. The following
code is taken from the 'CakeMapPlot.R' script.

Because the income dataset was produced for old ward boundaries (they were slightly
modified for the 2011 census), we cannot merge with the spatial dataset based on the
new zone codes. Instead we rely on the name of the wards. The code below provides a
snapshot of these names and demonstrates how they can be joined using
`inner_join`.

```{r, eval=FALSE}
wards@data <- join(wards@data, imd)
summary(imd$NAME %in% wards$NAME)
##       Mode   FALSE    TRUE    NA's 
##    logical      55      71       0 
```


The above code first joins the two datasets together and then checks the result by
seeing how many matches names there are.
In practice the fit between old names and new
names is quite poor: only 71 out of 124. In a
proper analysis we would have to solve this
problem (e.g. via the command `pmatch`, which stands for partial match).
For the purposes of this exercise we will simply plot income against
simulated cake consumption to gain a feeling what
it tells us about the relationship between cake consumption and wealth.

![](figures/incomeCake.png)

**Scatterplot** illustrating the
relationship between modelled average ward income and
 simulated number of cakes eaten per person per week.

The question raised by this finding is: why? 
Not why is cake consumption higher in wealthy areas (this has not
been established) but: why has the model resulted in this correlation?
To explore this question we need to go back and look at the individual
level data. The most relevant constraint variable for income was class.
When we look at the relationship between social class and cake consumption
in the Dental Health Survey, we find that there is indeed a link:
individuals in the highest three classes (1.1, 1.2, 2) have an average
cake intake of 3.9 cakes per week whereas the three lowest classes have
an average intake of 3.7. This is a relatively modest difference but,
when averaging over large areas, it helps explain the result.

As a bonus exercise, explore
the class dependence of cake consumption in the Dental Health Survey.

\clearpage

# Spatial microdata in agent-based models

In some ways, we can see spatial microsimulation as a precursor to, or early
form of, agent-based models (ABMs). Agent-based modelling depends on 1) a number of
discrete agents, 2) with different characteristics, 3) interacting. With the
spatial microsimulation model created in the last chapter we have 2 of these
3 elements of an ABMs: if your aim is to use spatial microdata as an input into
agent based models, you're half way there!

We do not have space in this tutorial to describe the transition from
the spatial microdata we have generated into a full ABM. Suffice to
mention some of the tools that will be useful for the job.

NetLogo is an mature and widely used toolkit for
agent-based models written in Java.
The recently published **RNetLogo** package provides an interface between
R and NetLogo, allowing for model runs to be set-up and run directly from
within R ([Thiele, 2014](http://www.jstatsoft.org/v58/i02/paper)). Because
much of the time taken for agent based modelling is focussed on analysis,
running NetLogo from within R, where the results can be saved, is
an eminently sensible choice.

Watch this space for more on this!

https://github.com/Robinlovelace/spatial-microsim-book

# Appendix: Getting up-to-speed with R {#apR}

As mentioned in [Chapter 1](#Introduction), R is a general purpose programming
language focussed on data analysis and modelling.
This small tutorial aims to teach the basics of R, from the perspective
of spatial microsimulation research.
It should also be useful to people with existing R skills, to
re-affirm their knowledge base and see how it is applicable to
spatial microsimulation.

R's design is built on the idea that "everything is an object and everything
that happens is a function". It is a *vectorised*,
*object orientated* and *functional* 
programming language
([Wickham, 2014](http://www.crcpress.com/product/isbn/9781466586963)).
This means that
R understands vector algebra, all data accessible to
R resides in a number of named objects and that a function must
be used to modify any object. We will look at each of these in some code below.

## R understands vector algebra

A vector is simply an ordered list of numbers (Beezer, 2008).
Imagine two vectors, each consisting of 3 elements:

$$a = (1,2,3); b = (9,8,6) $$

To say that R understands vector algebra is to say that it knows how to
handle vectors in the same way a mathematician does: 

$$a + b = (a_1 + b_1, a_2 + b_2, c_3 + c_3  ) = (10,10,9) $$

This may not seem remarkable, but it is. Most programming
languages are not vectorised, so they would see $a + b$ differently.
In Python, for example, this is the answer we get:^[We can
get the right answer in Python, by typing the following:
`import numpy; a=numpy.array([1,2,3]); b=numpy.array([9,8,6]); a+b`.]

```{r, engine='python', eval=FALSE}
a = [1,2,3]
b = [9,8,6]
print(a + b)
```

`## [1, 2, 3, 9, 8, 6]`

In R, the operation *just works*, intuitively:

```{r}
a <- c(1, 2, 3)
b <- c(9, 8, 6)
a + b
```

This conciseness is clearly very useful in spatial microsimulation,
as numeric variables of the same length are common (e.g. the attributes
of individuals in a zone) and can be acted on with a minimum
of effort.

## R is object orientated

In R, everything that exists is an object with a name
and a class. This is useful, because R's functions
know automatically how to behave differently on
different objects depending on their class.

To illustrate the point, let's create two objects, each with a different
class and see how the function `summarise` behaves differently, depending
on the type. This behavior is *polymorphism* (Matloff, 2011):

```{r}
# Create a character and a vector object
char_obj <- c("red", "blue", "red", "green")
num_obj <- c(1, 4, 2, 532.1)

# Summary of each object
summary(char_obj)
summary(num_obj)

# Summary of a factor object
fac_obj <- factor(char_obj)
summary(fac_obj)
```

In the example above, the output from `summary` for the numeric object `num_obj`
was very different from that of
the character vector `char_obj`. Note that although
the same information was contained in `fac_obj` (a factor), the output
from `summary` changes again.

Note that objects can be called almost anything in R with the exceptions
of names beginning with a number or containing operator symbols such as `-`,
`^` and brackets. It is good practice to think about what the purpose of
an object is before naming it: using clear and concise names can save you
a huge amount of time in the long run.


## Subsetting in R {#subsetting}

R has powerful, concise and (over time) intuitive methods for taking
subsets of data. Using the SimpleWorld example we loaded in *[Data preparation](#DataPrep)*,
let's explore the `ind` object in more detail, to see how we can select
the parts of an object we are most interested in. As before, we need
to load the data:

```{r}
ind <- read.csv("data/SimpleWorld/ind.csv") 
```

Now, it is easy from within R to
call a single individual (e.g. individual 3) using the square
bracket notation:

```{r}
ind[3,]
```

The above example takes a subset of `ind` all elements present on the 3rd row:
for a 2 dimensional table, anything to the left of the comma refers to rows and
anything to the right refers to columns. Note that `ind[2:3,]` and `ind[c(3,5),]`
also take subsets of the `ind` object: the square brackets can take *vector* inputs
as well as single numbers.

We can also subset by columns: the second dimension. Confusingly, this can be done in
four ways, because `ind` is an R `data.frame`^[This
can be ascertained by typing `class(ind)`. It is useful to know
the class of different R objects, so make good use of the `class()` function.]
and a data frame can behave simultaneously as
a list, a matrix and a data frame (only the results of the first are shown):

```{r}
ind$age # data.frame column name notation I
# ind[, 2] # matrix notation
# ind["age"] # column name notation II
# ind[[2]] # list notation
# ind[2] # numeric data frame notation
```

It is also possible to subset cells by both rows and columns simultaneously.
Let us select query the gender of the 4th individual, as an example
(pay attention to the relative location of the comma inside the square brackets):

```{r}
ind[4, 3] # The attribute of the 4th individual in column 3
```

A commonly used trick in R that helps with the analysis of individual-level
data is to subset a data frame
based on one or more of its variables. Let's subset first all females
in our dataset and then all females over 50:

```{r}
ind[ind$sex == "f", ]
ind[ind$sex == "f" & ind$age > 50, ]
```

In the above code, R uses relational operators of equality (`==`)
and inequality (`>`) which can be used in combination using
the `&` symbol. This works because,
as well as integer numbers, one can also place *boolean* variables
into square brackets: `ind$sex == "f"` returns a binary vector consisting
solely of `TRUE` and `FALSE` values.^[Thus, yet another way to invoke the 2nd column of
`ind` is the following: `ind[c(F, T, F)]`! Here, `T` and `F` are shorthand
for "TRUE" and "FALSE" respectively.] 

### Further R resources {#further}

The above tutorial should provide a sufficient grounding in R
for beginners to understand the practical examples in the book.
However, R is a deep language and there is much else to learn that will
be of benefit to your modelling skills. The following resources are
highly recommended:

- *An Introduction to R* (Venables et al., 2014)
is the foundational introductory R manual, written by the
software's core developers. It is terse and covers some advanced topics, but
provides an unbeatable introduction to R's behaviour as a language.
- *Advanced R* 
([Wickham, 2014](http://www.crcpress.com/product/isbn/9781466586963))
delves into the heart
of the R language. It contains many advanced topics, but the introductory
chapters are straightforward. Browsing some of the pages on
[Advanced R's website](http://adv-r.had.co.nz/) and
trying to answer the questions that open each chapter is an excellent
way of testing and improving one's understanding of R.
- *Introduction to visualising spatial data in R* (Lovelace and Cheshire, 2014)
provides an introductory tutorial on handling spatial data in R, including the
administrative zone data which often form the building blocks of spatial microsimulation
models in R.

There are alternatives to R and in the next section we will consider a few of these.

\clearpage

# References

Beezer, R. A. (2008). A first course in linear algebra. Puget Sound: Congruent Press.

Edwards, K. L., Clarke, G. P., Thomas, J., & Forman, D. (2010). Internal and External Validation of Spatial Microsimulation Models: Small Area Estimates of Adult Obesity. Applied Spatial Analysis and Policy, 4(4), 281–300. doi:10.1007/s12061-010-9056-2

Feinerer, I. (2013). Introduction to the tm Package Text Mining in R. Comprehensive R Archive Network, 1–8.

Harland, K. (2013). Microsimulation model user guide: flexible modelling framework (No. 0613). Leeds. Retrieved from http://eprints.ncrm.ac.uk/3177/
Knoblauch, K., & Maloney, L. T. (2012). Modeling psychophysical data in R (Vol. 32). Springer.

Lovelace, R., & Ballas, D. (2013). “Truncate, replicate, sample”: A method for creating integer weights for spatial microsimulation. Computers, Environment and Urban Systems, 41, 1–11. doi:10.1016/j.compenvurbsys.2013.03.004

Lovelace, R., & Cheshire, J. (2014). Introduction to visualising spatial data in R. National Centre for Research Methods, 14(03). Retrieved from http://eprints.ncrm.ac.uk/3295/

Meyer, D., Hornik, K., & Feinerer, I. (2008). Text mining infrastructure in R. Journal of Statistical Software, 25(5). Retrieved from http://epub.wu.ac.at/3978/

Pritchard, D. R., & Miller, E. J. (2012). Advances in population synthesis: fitting many attributes per agent and fitting to household and person margins simultaneously. Transportation, 39(3), 685–704. doi:10.1007/s11116-011-9367-4

Rey, S. J. (2014). Open regional science. The Annals of Regional Science, 52(3), 825–837. doi:10.1007/s00168-014-0611-7

Thiele, J. (2014). R Marries NetLogo: Introduction to the RNetLogo Package. Journal of Statistical, 58(2), 1–41. Retrieved from http://www.jstatsoft.org/v58/i02/paper

Venables, W. W. N., Smith, D. M. D. M., Team, R. D. C., & others. (2014). An introduction to R, 1. Retrieved from http://folk.uio.no/bernde/pub/Statistics.pdf
Wickham, H. (2014). Advanced R. Retrieved from http://www.crcpress.com/product/isbn/9781466586963

Xie, Y. (2013). Animation: an R package for creating animations and demonstrating statistical methods. Journal of Statistical Software, 53(1), 1–27.

